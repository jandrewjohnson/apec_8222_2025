{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# OLS Bias–Variance Goldilocks Demo + per-degree fit visuals\n",
                "# This extends your working snippet by adding a Goldilocks plot (train/test MSE vs degree)\n",
                "# while keeping the multiple fitted-curve figures.\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# --- Data (same setup you used) ---\n",
                "np.random.seed(1111)\n",
                "n = 100\n",
                "X = np.random.uniform(-3, 3, size=(n, 1))\n",
                "y_true = np.sin(1.5 * X) + 0.5 * np.cos(3 * X)\n",
                "y = y_true + np.random.normal(0, 0.6, size=y_true.shape)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=11)\n",
                "\n",
                "# --- 1) Goldilocks plot: train/test MSE vs degree ---\n",
                "# Evaluate a broader range of model complexities\n",
                "max_degree_eval = min(23, n // 2)  # cap to avoid degenerate very-high fits\n",
                "degrees_eval = list(range(1, max_degree_eval + 1))\n",
                "\n",
                "train_err, test_err = [], []\n",
                "for d in degrees_eval:\n",
                "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
                "    Xtr_poly = poly.fit_transform(X_train)\n",
                "    Xte_poly = poly.transform(X_test)\n",
                "    model = LinearRegression().fit(Xtr_poly, y_train.ravel())\n",
                "    ytr_pred = model.predict(Xtr_poly)\n",
                "    yte_pred = model.predict(Xte_poly)\n",
                "    train_err.append(mean_squared_error(y_train, ytr_pred))\n",
                "    test_err.append(mean_squared_error(y_test, yte_pred))\n",
                "\n",
                "best_deg = degrees_eval[int(np.argmin(test_err))]\n",
                "\n",
                "plt.figure(figsize=(8, 5))\n",
                "plt.plot(degrees_eval, train_err, marker='o', label=\"Train error\")\n",
                "plt.plot(degrees_eval, test_err, marker='o', label=\"Test error\")\n",
                "plt.axvline(best_deg, linestyle='--', label=\"Goldilocks zone\")\n",
                "plt.xlabel(\"Polynomial degree (model complexity)\")\n",
                "plt.ylabel(\"Mean Squared Error\")\n",
                "plt.title(\"Bias–Variance Tradeoff in OLS (Goldilocks Effect)\")\n",
                "plt.legend()\n",
                "plt.grid(True)\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# --- 2) Visualize OLS polynomial fits at multiple complexities ---\n",
                "# Degrees to show - log space up to n/2\n",
                "degrees_to_show = np.logspace(0, np.log10(n/2), num=8, dtype=int)\n",
                "degrees_to_show = np.unique(degrees_to_show)  # Remove duplicates\n",
                "degrees_to_show =[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]  # Remove duplicates\n",
                "\n",
                "# Prediction grid for smooth curves\n",
                "x_min, x_max = X.min() - 0.5, X.max() + 0.5\n",
                "x_plot = np.linspace(x_min, x_max, 400).reshape(-1, 1)\n",
                "\n",
                "for d in degrees_to_show:\n",
                "    poly = PolynomialFeatures(degree=d, include_bias=False)\n",
                "    Xtr_poly = poly.fit_transform(X_train)\n",
                "    Xpl_poly = poly.transform(x_plot)\n",
                "    model = LinearRegression().fit(Xtr_poly, y_train.ravel())\n",
                "    y_plot = model.predict(Xpl_poly)\n",
                "    \n",
                "    # Compute train/test MSE for the title\n",
                "    ytr_pred = model.predict(Xtr_poly)\n",
                "    yte_pred = model.predict(poly.transform(X_test))\n",
                "    tr_mse = mean_squared_error(y_train, ytr_pred)\n",
                "    te_mse = mean_squared_error(y_test, yte_pred)\n",
                "\n",
                "    # Dynamic y-limits centered on data (prevents extreme blow-ups from dominating)\n",
                "    y_data_min = min(y_train.min(), y_test.min())\n",
                "    y_data_max = max(y_train.max(), y_test.max())\n",
                "    y_range = y_data_max - y_data_min\n",
                "    y_center = (y_data_max + y_data_min) / 2\n",
                "    y_half_range = y_range * 0.75  # show 150% of the data's range\n",
                "    y_min = y_center - y_half_range\n",
                "    y_max = y_center + y_half_range\n",
                "\n",
                "    plt.figure(figsize=(7,5))\n",
                "    plt.scatter(X_train, y_train, s=30, alpha=0.8, label=\"Train points\")\n",
                "    plt.scatter(X_test, y_test, s=40, alpha=0.9, marker='x', label=\"Test points\")\n",
                "    plt.plot(x_plot, y_plot, linewidth=2, label=f\"OLS fit (degree={d})\")\n",
                "    plt.title(f\"Polynomial OLS fit (degree={d})\\nTrain MSE={tr_mse:.3f} | Test MSE={te_mse:.3f}\")\n",
                "    plt.xlabel(\"X\")\n",
                "    plt.ylabel(\"y\")\n",
                "    plt.ylim(y_min, y_max)\n",
                "    plt.legend()\n",
                "    plt.grid(True)\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": [
        {
            "kernelspec": {
                "display_name": "Python 3 (ipykernel)",
                "language": "python",
                "name": "python3"
            }
        }
    ],
    "nbformat": 4,
    "nbformat_minor": 4
}
